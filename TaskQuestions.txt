Who is it? Where was it published?
Why is it relevant? Why did they write it?
 - Accuracy and 
Main Contribution
Background, why is it novel
Data Sets
 - Training Data: Wikipedia Dumps in 9 languages (Arabic, Czech, German, English, Spanish, French, Italian, Romanian, Russian)
 - Test Data: 
	- English word analogy: http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt
Tasks
 - Word Representation Tasks (Encoding)
Approach, Baselines, Methods
 - Baseline: 
	- Human Similarity - using the cosine similarity between words to compare to human graded "relatedness" (Spearman's Rank Correlation)
	- Word Analogy - 
	- Morphological - Luong, Qiu, Soricut and Och, Botha and Blumson
	- Effect of size of training data - 
	- Effect of size of n-gram - 
	- Language modeling - 
Notable Tricks
Measurability
Quantitative Results
Qualitative Results
Is there an error analysis?
 - No? (confirmed by BW)  they just show why theirs is better which is a big problem with how 
Whatâ€™s next?
Do you believe it? Do I see problems?
Where do they cheat?
Can you apply it? Is code and data available? 
 - https://github.com/facebookresearch/fastText
What are weaknesses?
 - This method lacks the contextual information gathered in the RNN or Transformer models
Impactful? Are there any Enterprise solutions that apply it?
Who cited it? Semantic Scholar
 - https://www.semanticscholar.org/paper/Enriching-Word-Vectors-with-Subword-Information-Bojanowski-Grave/e2dba792360873aef125572812f3673b1a85d850





What is the difference between semantic and syntactic measure for word analogy baseline dataset?
 - definition is difficult
 - See email from B. Winter
 
What does equation in 6.2 mean? How do they weight certain n-grams?

Benjamin Winter Expectations for Presentation:
-Must feel that we have understood the paper
-Why is the paper important (how does this new contribution improve the state of the art)  How has it made things worse?
-How is it used now?  (recommend looking at papers that reference this and improve upon it)

BW Notes: Videoconference 19.05.2020
This model is important because of how well it models rare or never seen words
Recommend covering more of why the model is important, rather than covering all of the different comparisons to baselines
Compare this model to the model in http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf.  And mention these types of "papers of the time" in the presentation.  Also make sure to mention the skipgram model in general as a precursor to this paper. These are the bases for this paper.
Recommend including the skipgram equation or the scoring function on Pg.3.